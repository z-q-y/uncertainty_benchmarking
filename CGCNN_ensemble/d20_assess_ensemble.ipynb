{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically search for an NVIDIA GPU and use it. If not, then use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.append('/global/u2/q/qingyanz/cgcnn')\n",
    "\n",
    "# Find and use the appropriate GPU/CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all of our preprocessed data from the caches that we generated with the `../preprocessing/create_sdt.py` and `../preprocessing/split_data.ipynb` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the data split from our Jupyter notebook cache\n",
    "with open('../preprocessing/sdt/feature_dimensions.pkl', 'rb') as file_handle:\n",
    "    orig_atom_fea_len, nbr_fea_len = pickle.load(file_handle)\n",
    "\n",
    "with open('../preprocessing/splits.pkl', 'rb') as file_handle:\n",
    "    splits = pickle.load(file_handle)\n",
    "\n",
    "sdts_train, sdts_val = splits['sdts_train'], splits['sdts_val']\n",
    "targets_train, targets_val = splits['targets_train'], splits['targets_val']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module because the following parameters were re-set: atom_fea_len, classification, h_fea_len, n_conv, n_h, nbr_fea_len, orig_atom_fea_len.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss    cp      dur\n",
      "-------  ------------  ------------  ----  -------\n",
      "      1        \u001b[36m0.4572\u001b[0m        \u001b[32m0.4832\u001b[0m     +  19.4035\n",
      "      2        \u001b[36m0.3464\u001b[0m        \u001b[32m0.4330\u001b[0m     +  16.4921\n",
      "      3        \u001b[36m0.2995\u001b[0m        \u001b[32m0.3586\u001b[0m     +  16.4730\n",
      "      4        \u001b[36m0.2926\u001b[0m        \u001b[32m0.3326\u001b[0m     +  16.5412\n",
      "      5        0.2953        \u001b[32m0.3225\u001b[0m     +  16.5594\n",
      "      6        0.2999        0.3255        16.4636\n",
      "      7        0.2939        \u001b[32m0.3044\u001b[0m     +  16.5367\n",
      "      8        \u001b[36m0.2635\u001b[0m        0.3085        16.4812\n",
      "      9        \u001b[36m0.2410\u001b[0m        0.3248        16.5095\n",
      "     10        \u001b[36m0.2374\u001b[0m        0.3131        16.4991\n",
      "     11        0.2459        0.3416        16.5183\n",
      "     12        0.2549        \u001b[32m0.2954\u001b[0m     +  16.6317\n",
      "     14        0.2321        \u001b[32m0.2872\u001b[0m     +  16.6780\n",
      "     15        0.2265        0.2943        16.4764\n",
      "     16        0.2236        0.2933        16.5036\n",
      "     17        0.2239        0.3074        16.5004\n",
      "     18        0.2175        0.2921        16.4860\n",
      "     19        0.2117        \u001b[32m0.2758\u001b[0m     +  16.4578\n",
      "     20        0.2270        0.3265        16.4459\n",
      "     21        0.2095        0.2833        16.5892\n",
      "     22        0.1940        0.2985        16.4510\n",
      "     23        0.2029        0.2954        16.4513\n",
      "     24        0.1965        \u001b[32m0.2736\u001b[0m     +  16.4994\n",
      "     25        0.2015        0.2802        16.6348\n",
      "     26        0.1898        0.2971        16.5730\n",
      "     27        0.1958        0.2762        16.6061\n",
      "     28        0.2059        0.2743        16.5482\n",
      "     29        0.1876        0.2893        16.5750\n",
      "     30        0.2192        0.2736        16.5540\n",
      "     31        0.1987        0.2868        16.5467\n",
      "     32        0.1996        \u001b[32m0.2721\u001b[0m     +  16.5447\n",
      "     33        0.1811        0.2862        16.5398\n",
      "     34        0.1791        \u001b[32m0.2697\u001b[0m     +  16.5496\n",
      "     35        0.1815        0.2810        16.5273\n",
      "     36        0.1890        0.2808        16.5785\n",
      "     37        0.1842        \u001b[32m0.2668\u001b[0m     +  16.5526\n",
      "     38        0.1909        0.2685        16.5317\n",
      "     39        0.1802        0.2894        16.5044\n",
      "     40        \u001b[36m0.1690\u001b[0m        0.2694        16.6087\n",
      "     41        \u001b[36m0.1415\u001b[0m        0.2685        16.5480\n",
      "     42        0.2143        0.2755        16.5241\n",
      "     43        0.1959        0.2699        16.5882\n",
      "     44        0.1687        0.2838        16.5437\n",
      "     45        0.1900        \u001b[32m0.2662\u001b[0m     +  16.4924\n",
      "     46        0.1702        \u001b[32m0.2631\u001b[0m     +  16.4463\n",
      "     47        0.1844        0.2698        16.5142\n",
      "     48        0.1958        \u001b[32m0.2602\u001b[0m     +  16.4319\n",
      "     49        0.1894        0.2695        16.4459\n",
      "     50        0.1733        0.2615        16.4807\n",
      "     51        0.1716        0.2651        16.4702\n",
      "     52        0.1875        0.2625        16.4406\n",
      "     53        0.1744        0.2887        16.5652\n",
      "     54        0.1767        0.2613        16.5735\n",
      "     55        0.1982        0.2662        16.5456\n",
      "     56        0.1799        0.2700        16.5877\n",
      "     57        0.1769        0.2707        16.5586\n",
      "     58        0.1691        0.2835        16.5528\n",
      "     59        0.1807        0.2617        16.5342\n",
      "     60        0.1811        \u001b[32m0.2566\u001b[0m     +  16.5636\n",
      "     61        0.1686        0.2574        16.5279\n",
      "     62        0.1880        0.2730        16.5760\n",
      "     63        0.1753        0.2605        16.5778\n",
      "     64        0.1784        0.2664        16.5503\n",
      "     65        0.1593        \u001b[32m0.2498\u001b[0m     +  16.6057\n",
      "     66        0.1796        0.2593        16.5994\n",
      "     67        0.1688        0.2925        16.5541\n",
      "     68        0.1617        0.2782        16.5307\n",
      "     69        0.1655        0.2683        16.7658\n",
      "     70        0.1607        0.2572        16.5562\n",
      "     71        0.1734        0.2760        16.5195\n",
      "     72        0.1763        0.2689        16.5394\n",
      "     73        0.1862        0.2750        16.5693\n",
      "     74        0.1648        0.2511        16.5031\n",
      "     75        0.1561        \u001b[32m0.2359\u001b[0m     +  16.5215\n",
      "     76        0.1712        0.2484        16.5576\n",
      "     77        0.1666        0.2417        16.5268\n",
      "     78        \u001b[36m0.1398\u001b[0m        0.2484        16.5261\n",
      "     79        0.1408        0.2531        16.5441\n",
      "     80        0.1460        0.2506        16.5424\n",
      "     81        0.1455        0.2563        16.5117\n",
      "     82        \u001b[36m0.1319\u001b[0m        0.2484        16.5728\n",
      "     83        0.1473        0.2540        16.5500\n",
      "     84        0.1378        0.2557        16.5078\n",
      "     85        0.1385        0.2530        16.5646\n",
      "     86        0.1399        0.2565        16.5334\n",
      "     87        0.1330        0.2387        16.5229\n",
      "     88        0.1753        0.2497        16.5347\n",
      "     89        0.1585        0.2516        16.5620\n",
      "     90        \u001b[36m0.1246\u001b[0m        0.2572        16.5287\n",
      "     91        0.1344        0.2457        16.5225\n",
      "     92        0.1256        0.2467        16.5509\n",
      "     93        0.1499        0.2449        16.5134\n",
      "     94        0.1323        0.2426        16.5027\n",
      "     95        0.1392        0.2401        16.5368\n",
      "     96        0.1281        0.2447        16.5320\n",
      "     97        0.1540        0.2486        16.5081\n",
      "     98        0.1379        0.2526        16.5370\n",
      "     99        0.1365        0.2509        16.5448\n",
      "    100        0.1485        0.2463        16.5100\n",
      "Re-initializing module because the following parameters were re-set: atom_fea_len, classification, h_fea_len, n_conv, n_h, nbr_fea_len, orig_atom_fea_len.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss    cp      dur\n",
      "-------  ------------  ------------  ----  -------\n",
      "      1        \u001b[36m0.3437\u001b[0m        \u001b[32m0.3310\u001b[0m     +  16.5489\n",
      "      2        \u001b[36m0.3030\u001b[0m        0.4175        16.5568\n",
      "      3        \u001b[36m0.2849\u001b[0m        \u001b[32m0.2549\u001b[0m     +  16.5372\n",
      "      4        \u001b[36m0.2627\u001b[0m        0.3071        16.5591\n",
      "      5        \u001b[36m0.2397\u001b[0m        \u001b[32m0.2385\u001b[0m     +  16.5959\n",
      "      6        \u001b[36m0.2387\u001b[0m        0.5964        16.5434\n",
      "      7        0.2629        0.3218        16.5351\n",
      "      8        0.2419        0.2645        16.5681\n",
      "      9        \u001b[36m0.2185\u001b[0m        0.3371        16.5594\n",
      "     10        0.2332        0.7224        16.5566\n",
      "     11        \u001b[36m0.2178\u001b[0m        0.4056        16.5740\n",
      "     12        0.2443        0.2548        16.5732\n",
      "     13        0.2259        0.3665        16.5671\n",
      "     14        0.2284        \u001b[32m0.1821\u001b[0m     +  16.5571\n",
      "     15        \u001b[36m0.2117\u001b[0m        \u001b[32m0.1573\u001b[0m     +  16.5813\n",
      "     16        0.2152        0.4630        16.5301\n",
      "     17        0.2247        0.2400        16.5598\n",
      "     18        0.2237        0.2042        16.6040\n",
      "     19        0.2259        0.1853        16.5463\n",
      "     20        0.2356        0.2001        16.5668\n",
      "     21        \u001b[36m0.2014\u001b[0m        0.3333        16.5660\n",
      "     22        \u001b[36m0.1737\u001b[0m        0.5194        16.5451\n",
      "     23        0.2136        0.2053        16.5357\n",
      "     24        0.1917        0.2284        16.5631\n",
      "     25        0.2023        0.2407        16.5641\n",
      "     26        0.2015        0.2896        16.5703\n",
      "     27        0.2251        0.2156        16.5639\n",
      "     28        0.2274        0.2057        16.5637\n",
      "     29        0.1802        0.1751        16.5447\n",
      "     30        0.1792        0.2160        16.5453\n",
      "     31        0.1909        0.2125        16.5780\n",
      "     32        \u001b[36m0.1639\u001b[0m        0.2117        16.5357\n",
      "     33        0.2348        0.1593        16.5416\n",
      "     34        0.1771        0.1754        16.5802\n",
      "     35        0.2098        0.1700        16.5378\n",
      "     36        0.1782        0.1960        16.5241\n",
      "     37        0.1917        \u001b[32m0.1491\u001b[0m     +  16.5733\n",
      "     38        0.2209        0.2225        16.5772\n",
      "     39        \u001b[36m0.1546\u001b[0m        0.1944        16.5305\n",
      "     40        0.1999        0.1816        16.5861\n",
      "     41        0.1829        0.1713        16.6851\n",
      "     42        0.1816        0.1887        16.5948\n",
      "     43        0.1777        0.1847        16.5629\n",
      "     44        \u001b[36m0.1421\u001b[0m        0.1715        16.5776\n",
      "     45        0.1745        0.1858        16.5829\n",
      "     46        0.1947        0.1749        16.5503\n",
      "     47        0.1978        0.1605        16.5993\n",
      "     48        0.1814        \u001b[32m0.1459\u001b[0m     +  16.5497\n",
      "     49        0.1644        0.1804        16.5698\n",
      "     50        0.1874        \u001b[32m0.1361\u001b[0m     +  16.5787\n",
      "     51        0.1779        0.1570        16.6141\n",
      "     52        0.1891        0.1428        16.5823\n",
      "     53        0.1733        0.1566        16.5917\n",
      "     54        0.1685        0.1701        16.5789\n",
      "     55        0.1769        0.4259        16.5751\n",
      "     56        0.1585        0.1904        16.5812\n",
      "     57        0.1637        0.1645        16.5746\n",
      "     58        0.2027        0.1554        16.5272\n",
      "     59        0.1884        0.1881        16.5433\n",
      "     60        0.1487        0.1705        16.5693\n",
      "     61        0.1665        0.1459        16.5697\n",
      "     62        0.1776        0.1602        18.6454\n",
      "     63        0.1590        0.1384        16.5528\n",
      "     64        0.1663        0.1573        16.5601\n",
      "     65        0.1478        0.1548        16.5500\n",
      "     66        0.1608        0.1637        16.5263\n",
      "     67        0.1605        0.1647        16.5759\n",
      "     68        0.1456        0.1826        16.5341\n",
      "     69        0.1592        0.2189        16.5272\n",
      "     70        0.1973        0.1559        16.5662\n",
      "     71        0.1761        0.1642        16.5573\n",
      "     72        0.1515        0.1857        16.5457\n",
      "     73        0.1842        0.1603        16.5418\n",
      "     74        0.1592        0.1567        16.5686\n",
      "     75        0.1582        0.1598        16.5455\n",
      "     76        0.1513        0.1951        16.5663\n",
      "     77        0.1733        0.1726        16.5734\n",
      "     78        \u001b[36m0.1413\u001b[0m        0.1562        16.5382\n",
      "     79        0.1492        0.1595        16.5364\n",
      "     80        \u001b[36m0.1347\u001b[0m        0.1629        16.5876\n",
      "     81        \u001b[36m0.1128\u001b[0m        0.1626        16.5734\n",
      "     82        0.1329        0.1598        16.5449\n",
      "     83        0.1275        0.1511        16.5879\n",
      "     84        0.1241        0.1401        16.5773\n",
      "     85        0.1501        0.1394        16.5324\n",
      "     86        0.1331        0.1587        16.5458\n",
      "     87        \u001b[36m0.1120\u001b[0m        0.1559        16.7830\n",
      "     88        0.1523        0.1514        16.5871\n",
      "     89        0.1298        0.1525        16.5510\n",
      "     90        0.1296        0.1529        16.5957\n",
      "     91        0.1238        0.1406        16.5586\n",
      "     92        0.1310        0.1481        16.5576\n",
      "     93        0.1353        0.1561        16.5745\n",
      "     94        0.1208        0.1494        16.5592\n",
      "     95        0.1250        0.1554        16.5336\n",
      "     96        0.1171        0.1532        16.6006\n",
      "     97        0.1287        0.1448        16.5634\n",
      "     98        0.1269        0.1465        16.5320\n",
      "     99        0.1356        0.1453        16.5637\n",
      "    100        0.1349        0.1563        16.5654\n",
      "Re-initializing module because the following parameters were re-set: atom_fea_len, classification, h_fea_len, n_conv, n_h, nbr_fea_len, orig_atom_fea_len.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss    cp      dur\n",
      "-------  ------------  ------------  ----  -------\n",
      "      1        \u001b[36m0.4855\u001b[0m        \u001b[32m0.3440\u001b[0m     +  16.5063\n",
      "      2        \u001b[36m0.3141\u001b[0m        \u001b[32m0.3435\u001b[0m     +  16.4990\n",
      "      3        \u001b[36m0.2948\u001b[0m        0.4408        16.5324\n",
      "      4        \u001b[36m0.2699\u001b[0m        \u001b[32m0.2841\u001b[0m     +  16.4996\n",
      "      5        \u001b[36m0.2623\u001b[0m        0.3180        16.5156\n",
      "      6        \u001b[36m0.2614\u001b[0m        0.6013        16.5109\n",
      "      7        0.2621        \u001b[32m0.2349\u001b[0m     +  16.4983\n",
      "      8        \u001b[36m0.2455\u001b[0m        0.7070        16.4805\n",
      "      9        0.2659        \u001b[32m0.2139\u001b[0m     +  16.5131\n",
      "     10        \u001b[36m0.2371\u001b[0m        0.2489        16.4996\n",
      "     11        0.2524        0.2276        16.4935\n",
      "     12        \u001b[36m0.1983\u001b[0m        0.2464        16.6300\n",
      "     13        0.2249        \u001b[32m0.1885\u001b[0m     +  16.7736\n",
      "     14        0.2045        0.2057        16.6338\n",
      "     15        0.2198        \u001b[32m0.1842\u001b[0m     +  16.5404\n",
      "     16        0.2162        \u001b[32m0.1698\u001b[0m     +  16.5643\n",
      "     17        0.1999        0.2209        16.5689\n",
      "     18        0.2316        0.2662        16.5144\n",
      "     19        0.2284        \u001b[32m0.1626\u001b[0m     +  16.5662\n",
      "     20        0.2201        0.1776        16.5262\n",
      "     21        0.2059        0.2113        16.4827\n",
      "     22        0.1995        0.1793        16.5170\n",
      "     23        \u001b[36m0.1941\u001b[0m        0.1832        16.5027\n",
      "     24        0.1980        0.1663        16.5704\n",
      "     25        0.2096        0.1891        16.5361\n",
      "     26        0.2022        0.2036        16.5228\n",
      "     27        \u001b[36m0.1916\u001b[0m        0.1720        16.5140\n",
      "     28        \u001b[36m0.1819\u001b[0m        0.2205        16.5243\n",
      "     29        0.2085        0.1931        16.5507\n",
      "     30        \u001b[36m0.1765\u001b[0m        0.1909        16.5018\n",
      "     31        0.2015        \u001b[32m0.1626\u001b[0m     +  16.5042\n",
      "     32        0.2116        \u001b[32m0.1358\u001b[0m     +  16.5401\n",
      "     33        0.1987        0.1675        16.4830\n",
      "     34        0.2093        0.1947        16.4831\n",
      "     35        0.2122        0.2146        16.5340\n",
      "     36        0.2013        0.2088        16.4888\n",
      "     37        0.2123        0.1879        16.4889\n",
      "     38        \u001b[36m0.1705\u001b[0m        0.2062        16.5117\n",
      "     39        0.2370        0.1974        16.5126\n",
      "     40        0.2031        0.1580        16.4830\n",
      "     41        0.1761        0.1650        16.5016\n",
      "     42        0.2306        0.2213        16.5123\n",
      "     43        0.1760        0.1609        16.4876\n",
      "     44        0.1770        0.1734        16.4878\n",
      "     45        0.2004        0.1600        16.5047\n",
      "     46        0.1745        0.1794        16.5031\n",
      "     47        0.1802        0.1785        16.4682\n",
      "     48        0.1766        0.1565        16.5194\n",
      "     49        0.1915        0.1790        16.4859\n",
      "     50        0.1935        0.2098        16.4753\n",
      "     51        0.1706        0.1886        16.5146\n",
      "     52        \u001b[36m0.1680\u001b[0m        0.2359        16.5061\n",
      "     53        0.1921        0.2056        16.4946\n",
      "     54        0.1773        0.1443        16.5085\n",
      "     55        0.1791        0.1758        16.5317\n",
      "     56        0.1792        0.1636        16.4961\n",
      "     57        0.1782        0.1743        16.4909\n",
      "     58        0.1786        0.1684        16.5186\n",
      "     59        0.1831        0.1402        16.5557\n",
      "     60        \u001b[36m0.1531\u001b[0m        0.1734        16.5206\n",
      "     61        0.1627        0.1631        16.5286\n",
      "     62        0.1772        0.1428        16.5035\n",
      "     63        0.1779        0.1920        16.4966\n",
      "     64        0.1732        0.1871        16.5238\n",
      "     65        0.1661        0.1530        16.5103\n",
      "     66        0.1728        0.1947        16.4995\n",
      "     67        0.1701        0.1644        16.4919\n",
      "     68        \u001b[36m0.1463\u001b[0m        0.1688        16.5105\n",
      "     69        0.1688        0.1789        16.4978\n",
      "     70        0.1532        0.1599        16.5276\n",
      "     71        0.1549        0.1505        16.5443\n",
      "     72        \u001b[36m0.1369\u001b[0m        0.1953        16.5000\n",
      "     73        0.1769        0.1825        16.5179\n",
      "     74        0.1743        0.1869        16.5199\n",
      "     75        0.1470        0.1791        16.5696\n",
      "     76        0.1517        0.1723        16.5055\n",
      "     77        0.1484        0.1603        16.5407\n",
      "     78        0.1516        0.1602        16.4988\n",
      "     79        0.1549        0.1536        16.4946\n",
      "     80        0.1524        0.1450        16.5266\n",
      "     81        0.1578        0.1690        16.5111\n",
      "     82        0.1432        0.1656        16.4851\n",
      "     83        0.1529        0.1475        16.5028\n",
      "     84        \u001b[36m0.1337\u001b[0m        0.1589        16.5280\n",
      "     85        0.1423        0.1517        16.5032\n",
      "     86        0.1485        0.1472        16.5018\n",
      "     87        \u001b[36m0.1305\u001b[0m        0.1678        16.5356\n",
      "     88        \u001b[36m0.1083\u001b[0m        0.1578        16.5132\n",
      "     89        0.1292        0.1676        16.4832\n",
      "     90        0.1386        0.1376        16.5343\n",
      "     91        0.1563        0.1720        16.5131\n",
      "     92        0.1315        0.1827        16.4847\n",
      "     93        0.1354        0.1880        16.5138\n",
      "     94        0.1287        0.1636        16.5177\n",
      "     95        0.1223        0.1541        16.5035\n",
      "     96        0.1458        0.1653        16.5083\n",
      "     97        0.1508        0.1521        16.5211\n",
      "     98        0.1227        0.1632        16.4921\n",
      "     99        0.1395        0.1605        16.4902\n",
      "    100        0.1522        0.1610        16.5100\n",
      "Re-initializing module because the following parameters were re-set: atom_fea_len, classification, h_fea_len, n_conv, n_h, nbr_fea_len, orig_atom_fea_len.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss    cp      dur\n",
      "-------  ------------  ------------  ----  -------\n",
      "      1        \u001b[36m0.4190\u001b[0m        \u001b[32m0.3293\u001b[0m     +  16.5771\n",
      "      2        \u001b[36m0.3340\u001b[0m        0.4002        16.5454\n",
      "      3        \u001b[36m0.2981\u001b[0m        0.3343        16.5656\n",
      "      4        \u001b[36m0.2738\u001b[0m        \u001b[32m0.2739\u001b[0m     +  16.5582\n",
      "      5        0.2746        0.5184        16.5378\n",
      "      6        \u001b[36m0.2625\u001b[0m        0.4105        16.5851\n",
      "      7        \u001b[36m0.2511\u001b[0m        \u001b[32m0.2603\u001b[0m     +  16.5966\n",
      "      8        \u001b[36m0.2288\u001b[0m        0.6535        16.6367\n",
      "      9        0.2489        0.2919        16.6073\n",
      "     10        0.2344        \u001b[32m0.1838\u001b[0m     +  16.6521\n",
      "     11        0.2785        0.2813        16.6061\n",
      "     12        0.2317        0.2093        16.6214\n",
      "     13        \u001b[36m0.2273\u001b[0m        \u001b[32m0.1696\u001b[0m     +  16.6395\n",
      "     14        \u001b[36m0.2236\u001b[0m        0.2901        16.6229\n",
      "     15        \u001b[36m0.2104\u001b[0m        0.1903        16.6161\n",
      "     16        0.2254        0.2536        16.6395\n",
      "     17        0.2448        0.2399        16.6197\n",
      "     18        0.2168        0.2028        16.6104\n",
      "     19        0.2141        0.1900        16.6578\n",
      "     20        0.2405        0.2031        16.6272\n",
      "     21        0.2310        0.2049        16.6073\n",
      "     22        0.2107        0.2169        16.6450\n",
      "     23        0.2209        0.1905        16.6399\n",
      "     24        \u001b[36m0.2010\u001b[0m        0.1794        16.6096\n",
      "     25        0.2232        0.1896        16.6381\n",
      "     26        0.2066        0.1860        16.6442\n",
      "     27        0.2287        0.1833        16.5947\n",
      "     28        0.2046        \u001b[32m0.1611\u001b[0m     +  16.6306\n",
      "     29        \u001b[36m0.1691\u001b[0m        0.1854        16.6418\n",
      "     30        0.1885        0.1815        16.6110\n",
      "     31        0.2009        \u001b[32m0.1483\u001b[0m     +  16.6241\n",
      "     32        0.2195        0.2305        16.6631\n",
      "     33        0.1930        0.1837        16.5931\n",
      "     34        0.1854        0.1973        16.5410\n",
      "     35        0.1846        0.1638        16.5811\n",
      "     36        0.1937        0.1646        16.5828\n",
      "     37        0.1959        0.1615        16.5451\n",
      "     38        0.2071        0.2127        16.5456\n",
      "     39        0.2185        0.1786        16.5970\n",
      "     40        0.2155        0.1694        16.5485\n",
      "     41        0.1733        0.1835        16.5395\n",
      "     42        0.1890        0.1695        16.5750\n",
      "     43        0.1790        0.2233        16.5903\n",
      "     44        0.1829        0.2038        16.5374\n",
      "     45        0.2056        0.1589        16.5648\n",
      "     46        \u001b[36m0.1678\u001b[0m        0.1933        16.5973\n",
      "     47        0.1795        0.1799        16.5754\n",
      "     48        0.1826        0.2082        16.5795\n",
      "     49        \u001b[36m0.1509\u001b[0m        0.1928        16.5990\n",
      "     50        0.1802        0.1730        16.5739\n",
      "     51        0.1645        0.1753        16.5410\n",
      "     52        0.1821        0.1721        16.5706\n",
      "     53        0.1791        0.1583        16.5664\n",
      "     54        0.1891        0.1519        16.5449\n",
      "     55        0.1758        0.2205        16.6000\n",
      "     56        0.1946        0.1591        16.5956\n",
      "     57        0.1592        \u001b[32m0.1460\u001b[0m     +  16.5824\n",
      "     58        0.1684        0.1647        16.5669\n",
      "     59        0.1768        0.1594        16.5840\n",
      "     60        0.1683        0.1548        16.5758\n",
      "     61        0.1815        0.1607        16.5503\n",
      "     62        0.1703        0.1763        16.6102\n",
      "     63        0.1750        0.1850        16.5949\n",
      "     64        0.1514        0.1835        16.5425\n",
      "     65        0.1956        0.1738        16.5569\n",
      "     66        0.1745        0.1691        16.6214\n",
      "     67        0.1672        0.1648        16.5671\n",
      "     68        0.1787        0.1777        16.5721\n",
      "     69        0.1755        0.1498        16.5826\n",
      "     70        0.1682        0.1746        16.6265\n",
      "     71        0.1700        0.1507        16.5614\n",
      "     72        0.1854        0.1765        16.5912\n",
      "     73        0.1749        0.1877        16.6184\n",
      "     74        0.1686        \u001b[32m0.1390\u001b[0m     +  16.5862\n",
      "     75        0.1816        \u001b[32m0.1388\u001b[0m     +  16.6101\n",
      "     76        0.1591        0.1799        16.5793\n",
      "     77        0.1586        0.1404        16.5900\n",
      "     78        0.1621        0.1520        16.5421\n",
      "     79        0.1562        0.1491        16.5934\n",
      "     80        0.1540        0.1514        16.5750\n",
      "     81        \u001b[36m0.1446\u001b[0m        \u001b[32m0.1300\u001b[0m     +  16.6572\n",
      "     82        \u001b[36m0.1422\u001b[0m        0.1428        16.5976\n",
      "     83        \u001b[36m0.1279\u001b[0m        0.1508        16.5813\n",
      "     84        0.1465        0.1400        16.5441\n",
      "     85        0.1318        0.1436        16.5822\n",
      "     86        0.1384        0.1589        16.5722\n",
      "     87        \u001b[36m0.1273\u001b[0m        0.1450        16.5506\n",
      "     88        0.1391        0.1358        16.5323\n",
      "     89        \u001b[36m0.1262\u001b[0m        0.1479        16.6360\n",
      "     90        0.1483        0.1363        16.5880\n",
      "     91        0.1598        0.1463        16.5438\n",
      "     92        0.1307        0.1410        16.5585\n",
      "     93        0.1364        0.1434        16.5893\n",
      "     94        0.1383        0.1459        16.5562\n",
      "     95        \u001b[36m0.1133\u001b[0m        0.1436        16.5627\n",
      "     96        0.1232        0.1560        16.5901\n",
      "     97        0.1264        0.1517        16.5761\n",
      "     98        \u001b[36m0.1088\u001b[0m        0.1398        16.5525\n",
      "     99        0.1753        0.1371        16.5450\n",
      "    100        0.1315        0.1496        16.5904\n",
      "Re-initializing module because the following parameters were re-set: atom_fea_len, classification, h_fea_len, n_conv, n_h, nbr_fea_len, orig_atom_fea_len.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss    cp      dur\n",
      "-------  ------------  ------------  ----  -------\n",
      "      1        \u001b[36m0.3945\u001b[0m        \u001b[32m0.3526\u001b[0m     +  16.4154\n",
      "      2        \u001b[36m0.3423\u001b[0m        0.4184        16.4056\n",
      "      3        \u001b[36m0.2925\u001b[0m        0.3715        16.4324\n",
      "      4        \u001b[36m0.2773\u001b[0m        \u001b[32m0.2399\u001b[0m     +  16.4733\n",
      "      5        \u001b[36m0.2538\u001b[0m        0.3093        16.4030\n",
      "      6        \u001b[36m0.2395\u001b[0m        \u001b[32m0.2164\u001b[0m     +  16.4042\n",
      "      7        0.2497        0.3227        16.4408\n",
      "      8        0.2400        0.2222        16.4144\n",
      "      9        0.2477        0.2446        16.3990\n",
      "     10        \u001b[36m0.2360\u001b[0m        0.2800        16.4335\n",
      "     11        0.2484        0.2695        16.4496\n",
      "     12        0.2391        \u001b[32m0.2016\u001b[0m     +  16.4228\n",
      "     13        \u001b[36m0.2163\u001b[0m        0.2645        16.4280\n",
      "     14        0.2276        0.2606        16.4426\n",
      "     15        \u001b[36m0.2160\u001b[0m        \u001b[32m0.1560\u001b[0m     +  16.4517\n",
      "     16        \u001b[36m0.2121\u001b[0m        0.2119        16.4032\n",
      "     17        0.2132        0.1762        16.4469\n",
      "     18        0.2418        0.3283        16.4604\n",
      "     19        0.2188        0.2246        16.4288\n",
      "     20        \u001b[36m0.2050\u001b[0m        0.1839        16.4189\n",
      "     21        0.2175        0.2109        16.4299\n",
      "     22        0.2088        0.1821        16.4138\n",
      "     23        \u001b[36m0.2023\u001b[0m        0.1690        16.4002\n",
      "     24        \u001b[36m0.1998\u001b[0m        0.2999        16.4026\n",
      "     25        0.2525        0.2746        16.4100\n",
      "     26        \u001b[36m0.1898\u001b[0m        0.1790        16.3955\n",
      "     27        0.2152        0.3274        16.3812\n",
      "     28        0.2149        0.1930        16.4322\n",
      "     29        0.2470        0.1845        16.4030\n",
      "     30        0.2013        0.1825        16.3793\n",
      "     31        0.1903        0.2292        16.4056\n",
      "     32        \u001b[36m0.1758\u001b[0m        0.2763        16.4083\n",
      "     33        0.1863        0.1970        16.4548\n",
      "     34        0.2086        0.2383        16.4125\n",
      "     35        0.1945        0.3508        16.4063\n",
      "     36        0.2037        0.1672        16.3942\n",
      "     37        0.2110        0.2074        16.4152\n",
      "     38        0.1865        0.2815        16.3939\n",
      "     39        \u001b[36m0.1683\u001b[0m        0.2224        16.3944\n",
      "     40        0.1780        0.2022        16.3893\n",
      "     41        0.2026        0.1903        16.4024\n",
      "     42        0.2095        0.1920        16.4497\n",
      "     43        0.2047        0.2288        16.4662\n",
      "     44        0.2082        0.2475        16.5850\n",
      "     45        0.1903        0.2143        16.4876\n",
      "     46        0.1940        0.2421        16.4550\n",
      "     47        0.2596        \u001b[32m0.1546\u001b[0m     +  16.4208\n",
      "     48        \u001b[36m0.1643\u001b[0m        0.2161        16.4281\n",
      "     49        0.2059        0.2064        16.4463\n",
      "     50        0.1771        0.2246        16.4637\n",
      "     51        0.1927        0.2095        16.4375\n",
      "     52        0.1818        0.1873        16.4345\n",
      "     53        0.1924        0.1666        16.4161\n",
      "     54        0.1662        0.1852        16.4636\n",
      "     55        \u001b[36m0.1561\u001b[0m        0.1833        16.4669\n",
      "     56        0.1820        0.1676        16.4202\n",
      "     57        0.1839        0.1791        16.4234\n",
      "     58        0.1740        0.1882        16.4634\n",
      "     59        0.1706        0.2146        16.4389\n",
      "     60        0.1987        0.2115        16.4322\n",
      "     61        0.1844        0.1915        16.4084\n",
      "     62        0.1745        0.1797        16.4676\n",
      "     63        0.1693        0.2914        16.4428\n",
      "     64        0.1744        0.1637        16.4157\n",
      "     65        0.1587        0.1797        18.1511\n",
      "     66        \u001b[36m0.1488\u001b[0m        0.1987        16.4637\n",
      "     67        0.1592        0.2320        16.4283\n",
      "     68        0.1500        0.2014        16.4204\n",
      "     69        0.1596        0.1751        16.4290\n",
      "     70        0.1669        0.1817        16.4688\n",
      "     71        0.1639        0.2132        16.4436\n",
      "     72        0.1856        0.2058        16.4300\n",
      "     73        0.1588        0.1874        16.4091\n",
      "     74        0.1504        0.2432        16.4534\n",
      "     75        0.1504        0.1604        16.4377\n",
      "     76        \u001b[36m0.1378\u001b[0m        0.2022        16.4131\n",
      "     77        0.1611        0.1916        16.4030\n",
      "     78        0.1444        0.2074        16.4838\n",
      "     79        \u001b[36m0.1301\u001b[0m        0.2084        16.4377\n",
      "     80        0.1370        0.2133        16.4291\n",
      "     81        0.1387        0.1917        16.4084\n",
      "     82        0.1468        0.1966        16.4769\n",
      "     83        0.1372        0.1949        16.4397\n",
      "     84        \u001b[36m0.1257\u001b[0m        0.1904        16.4056\n",
      "     85        0.1363        0.2066        16.4178\n",
      "     86        0.1504        0.1918        16.4544\n",
      "     87        0.1277        0.1949        16.4271\n",
      "     88        0.1404        0.1910        16.4216\n",
      "     89        0.1406        0.1987        16.4299\n",
      "     90        0.1442        0.1975        16.4759\n",
      "     91        0.1348        0.2048        16.4509\n",
      "     92        \u001b[36m0.1188\u001b[0m        0.1996        16.4246\n",
      "     93        0.1646        0.2044        16.4295\n",
      "     94        0.1447        0.1872        16.4690\n",
      "     95        0.1460        0.2008        16.4587\n",
      "     96        0.1313        0.2019        16.4228\n",
      "     97        0.1307        0.1970        16.4220\n",
      "     98        \u001b[36m0.1152\u001b[0m        0.2001        16.4483\n",
      "     99        0.1359        0.2111        16.4283\n",
      "    100        0.1420        0.2149        16.4327\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim import Adam\n",
    "import skorch.callbacks.base\n",
    "from skorch.callbacks import Checkpoint  # needs skorch >= 0.4\n",
    "from skorch.callbacks.lr_scheduler import LRScheduler\n",
    "from skorch import NeuralNetRegressor\n",
    "from cgcnn.dropoutmodel20 import CrystalGraphConvNet\n",
    "from cgcnn.data import collate_pool, MergeDataset\n",
    "\n",
    "\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('./histories/%i_valid_best_params.pt' % k)\n",
    "\n",
    "nets = []\n",
    "# Fold the CV data\n",
    "k_folder = KFold(n_splits=5)\n",
    "for k, (indices_train, _) in enumerate(k_folder.split(sdts_train)):\n",
    "    stds_train_ = [sdts_train[index] for index in indices_train]\n",
    "    targets_train_ = np.array([targets_train[index] for index in indices_train])\n",
    "\n",
    "    # Define various callbacks and checkpointers for this network\n",
    "    LR_schedule = LRScheduler('MultiStepLR', milestones=[75], gamma=0.1)\n",
    "    cp = Checkpoint(monitor='valid_loss_best', fn_prefix='./histories/%i_valid_best_' % k)\n",
    "    load_best_valid_loss = train_end_load_best_valid_loss()\n",
    "\n",
    "    # Train this fold's network\n",
    "    net = NeuralNetRegressor(\n",
    "        CrystalGraphConvNet,\n",
    "        module__orig_atom_fea_len=orig_atom_fea_len,\n",
    "        module__nbr_fea_len=nbr_fea_len,\n",
    "        batch_size=214,\n",
    "        module__classification=False,\n",
    "        lr=0.0056,\n",
    "        max_epochs=100,\n",
    "        module__atom_fea_len=46,\n",
    "        module__h_fea_len=83,\n",
    "        module__n_conv=8,\n",
    "        module__n_h=4,\n",
    "        optimizer=Adam,\n",
    "        iterator_train__pin_memory=True,\n",
    "        iterator_train__num_workers=0,\n",
    "        iterator_train__collate_fn=collate_pool,\n",
    "        iterator_train__shuffle=True,\n",
    "        iterator_valid__pin_memory=True,\n",
    "        iterator_valid__num_workers=0,\n",
    "        iterator_valid__collate_fn=collate_pool,\n",
    "        iterator_valid__shuffle=False,\n",
    "        device=device,\n",
    "        criterion=torch.nn.L1Loss,\n",
    "        dataset=MergeDataset,\n",
    "        callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    "    )\n",
    "    net.initialize()\n",
    "    net.fit(stds_train_, targets_train_)\n",
    "    nets.append(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading models\n",
    "It takes a few hours to fit the 5-model ensemble. You can either do it via notebook (above) or via `sbatch submit_ensemble_fitting.sh`. Either way, you load the results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim import Adam\n",
    "import skorch.callbacks.base\n",
    "from skorch.callbacks import Checkpoint  # needs skorch >= 0.4\n",
    "from skorch.callbacks.lr_scheduler import LRScheduler\n",
    "from skorch import NeuralNetRegressor\n",
    "from cgcnn.dropoutmodel20 import CrystalGraphConvNet\n",
    "from cgcnn.data import collate_pool, MergeDataset\n",
    "\n",
    "\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('./histories/%i_valid_best_params.pt' % k)\n",
    "\n",
    "nets = []\n",
    "# Fold the CV data\n",
    "k_folder = KFold(n_splits=5)\n",
    "for k, (indices_train, _) in enumerate(k_folder.split(sdts_train)):\n",
    "    stds_train_ = [sdts_train[index] for index in indices_train]\n",
    "    targets_train_ = np.array([targets_train[index] for index in indices_train])\n",
    "\n",
    "    # Define various callbacks and checkpointers for this network\n",
    "    LR_schedule = LRScheduler('MultiStepLR', milestones=[75], gamma=0.1)\n",
    "    cp = Checkpoint(monitor='valid_loss_best', fn_prefix='./histories/%i_valid_best_' % k)\n",
    "    load_best_valid_loss = train_end_load_best_valid_loss()\n",
    "\n",
    "    # Train this fold's network\n",
    "    net = NeuralNetRegressor(\n",
    "        CrystalGraphConvNet,\n",
    "        module__orig_atom_fea_len=orig_atom_fea_len,\n",
    "        module__nbr_fea_len=nbr_fea_len,\n",
    "        batch_size=214,\n",
    "        module__classification=False,\n",
    "        lr=0.0056,\n",
    "        max_epochs=100,\n",
    "        module__atom_fea_len=46,\n",
    "        module__h_fea_len=83,\n",
    "        module__n_conv=8,\n",
    "        module__n_h=4,\n",
    "        optimizer=Adam,\n",
    "        iterator_train__pin_memory=True,\n",
    "        iterator_train__num_workers=0,\n",
    "        iterator_train__collate_fn=collate_pool,\n",
    "        iterator_train__shuffle=True,\n",
    "        iterator_valid__pin_memory=True,\n",
    "        iterator_valid__num_workers=0,\n",
    "        iterator_valid__collate_fn=collate_pool,\n",
    "        iterator_valid__shuffle=False,\n",
    "        device=device,\n",
    "        criterion=torch.nn.L1Loss,\n",
    "        dataset=MergeDataset,\n",
    "        callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    "    )\n",
    "    net.initialize()\n",
    "    net.load_params(f_history='./histories/%i_valid_best_history.json' % k,\n",
    "                    f_optimizer= './histories/%i_valid_best_optimizer.pt' % k, \n",
    "                    f_params='./histories/%i_valid_best_params.pt' % k)\n",
    "    nets.append(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference - net.fit() vs net.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling\n",
    "Wrap the five networks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble:\n",
    "    def __init__(self, networks):\n",
    "        self.networks = networks\n",
    "\n",
    "    def predict(self, features):\n",
    "        for net in self.networks:\n",
    "            prediction = net.predict(features)\n",
    "            try:\n",
    "                predictions = np.hstack((predictions, prediction))\n",
    "            except NameError:\n",
    "                predictions = prediction\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "ensemble = Ensemble(nets)\n",
    "preds = ensemble.predict(sdts_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ensemble' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8ebd24ce3634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Make the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdts_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtargets_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mresiduals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtargets_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ensemble' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "# Make the predictions\n",
    "predictions = ensemble.predict(sdts_val)\n",
    "targets_pred = predictions.mean(axis=1)\n",
    "residuals = targets_pred - targets_val.reshape(-1)\n",
    "standard_errors = predictions.std(axis=1)\n",
    "\n",
    "# Calculate the error metrics\n",
    "mae = mean_absolute_error(targets_val, targets_pred)\n",
    "rmse = np.sqrt(mean_squared_error(targets_val, targets_pred))\n",
    "r2 = r2_score(targets_val, targets_pred)\n",
    "\n",
    "# Report\n",
    "print('MAE = %.2f eV' % mae)\n",
    "print('RMSE = %.2f eV' % rmse)\n",
    "print('R^2 = %.2f' % r2)\n",
    "\n",
    "\"\"\"\n",
    "# Save as pickle to be plotted with in the same graph as others\n",
    "with open('assess_ensemble_plots_d20.pkl', 'wb') as saveplot:\n",
    "    pickle.dump((predictions, targets_val), saveplot)\n",
    "\"\"\"\n",
    "    \n",
    "# Plot\n",
    "lims = [-4, 2]\n",
    "grid = sns.jointplot(targets_val.reshape(-1), targets_pred,\n",
    "                     kind='hex',\n",
    "                     bins='log',\n",
    "                     extent=lims+lims)\n",
    "_ = grid.ax_joint.set_xlim(lims)\n",
    "_ = grid.ax_joint.set_ylim(lims)\n",
    "_ = grid.ax_joint.plot(lims, lims, '--')\n",
    "_ = grid.ax_joint.set_xlabel('DFT $\\Delta$E [eV]')\n",
    "_ = grid.ax_joint.set_ylabel('CGCNN $\\Delta$E [eV]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a732130c14497fac2b8866510a26ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Calibration', max=20, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "def calculate_density(percentile):\n",
    "    num_within_quantile = 0\n",
    "    for se, resid in zip(standard_errors, residuals):\n",
    "        norm = stats.norm(loc=0, scale=se)\n",
    "        lower_bound = norm.ppf(0.5-percentile/2)\n",
    "        upper_bound = norm.ppf(0.5+percentile/2)\n",
    "        if lower_bound <= resid <= upper_bound:\n",
    "            num_within_quantile += 1\n",
    "    density = num_within_quantile / len(residuals)\n",
    "    return density\n",
    "\n",
    "predicted_pi = np.linspace(0, 1, 20)\n",
    "observed_pi = [calculate_density(quantile)\n",
    "               for quantile in tqdm_notebook(predicted_pi, desc='Calibration')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration factor = -0.17\n",
      "Sharpness = 0.14 eV\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import integrate\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Plot the calibration curve\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax_ideal = sns.lineplot([0, 1], [0, 1], label='ideal')\n",
    "_ = ax_ideal.lines[0].set_linestyle('--')\n",
    "ax_gp = sns.lineplot(predicted_pi, observed_pi, label='GP')\n",
    "_ = ax_gp.set_xlabel('Expected prediction interval')\n",
    "_ = ax_gp.set_ylabel('Observed prediction interval')\n",
    "_ = ax_gp.set_xlim([0, 1])\n",
    "_ = ax_gp.set_ylim([0, 1])\n",
    "\n",
    "# Report the calibration factor\n",
    "actual_calibration_area = integrate.trapz(y=observed_pi, x=predicted_pi)\n",
    "ideal_calibration_area = integrate.trapz(y=predicted_pi, x=predicted_pi)\n",
    "calibration_factor = actual_calibration_area - ideal_calibration_area\n",
    "print('Calibration factor = %.2f' % calibration_factor)\n",
    "\n",
    "# Report sharpness\n",
    "sharpness = np.sqrt(np.mean(standard_errors**2))\n",
    "print('Sharpness = %.2f eV' % sharpness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaspy_ktran",
   "language": "python",
   "name": "gaspy_ktran"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
