{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically search for an NVIDIA GPU and use it. If not, then use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.append('/global/u2/q/qingyanz/cgcnn')\n",
    "\n",
    "# Find and use the appropriate GPU/CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Using GPU')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all of our preprocessed data from the caches that we generated with the `../preprocessing/create_sdt.py` and `../preprocessing/split_data.ipynb` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the data split from our Jupyter notebook cache\n",
    "with open('../preprocessing/sdt/feature_dimensions.pkl', 'rb') as file_handle:\n",
    "    orig_atom_fea_len, nbr_fea_len = pickle.load(file_handle)\n",
    "\n",
    "with open('../preprocessing/splits.pkl', 'rb') as file_handle:\n",
    "    splits = pickle.load(file_handle)\n",
    "\n",
    "sdts_train, sdts_val = splits['sdts_train'], splits['sdts_val']\n",
    "targets_train, targets_val = splits['targets_train'], splits['targets_val']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module because the following parameters were re-set: atom_fea_len, classification, h_fea_len, n_conv, n_h, nbr_fea_len, orig_atom_fea_len.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss    cp      dur\n",
      "-------  ------------  ------------  ----  -------\n",
      "      1        \u001b[36m0.5293\u001b[0m        \u001b[32m0.4167\u001b[0m     +  17.4857\n",
      "      2        \u001b[36m0.3417\u001b[0m        \u001b[32m0.3865\u001b[0m     +  17.0975\n",
      "      3        \u001b[36m0.2956\u001b[0m        0.4480        17.1506\n",
      "      4        \u001b[36m0.2567\u001b[0m        0.4553        17.1911\n",
      "      5        \u001b[36m0.2388\u001b[0m        \u001b[32m0.3164\u001b[0m     +  17.1093\n",
      "      6        \u001b[36m0.2366\u001b[0m        0.4058        17.1371\n",
      "      7        \u001b[36m0.2252\u001b[0m        0.3746        17.1093\n",
      "      8        0.2316        0.3461        17.1641\n",
      "      9        \u001b[36m0.2227\u001b[0m        0.3422        17.1383\n",
      "     10        0.2289        0.3954        17.0766\n",
      "     11        \u001b[36m0.2223\u001b[0m        0.3576        17.2198\n",
      "     12        \u001b[36m0.2024\u001b[0m        0.3223        17.0767\n",
      "     13        0.2235        \u001b[32m0.2953\u001b[0m     +  17.0894\n",
      "     14        0.2457        0.3695        17.1211\n",
      "     15        0.2064        0.4035        17.1767\n",
      "     16        \u001b[36m0.1972\u001b[0m        \u001b[32m0.2916\u001b[0m     +  17.0747\n",
      "     17        0.2093        0.3243        17.1522\n",
      "     18        \u001b[36m0.1956\u001b[0m        \u001b[32m0.2727\u001b[0m     +  17.1279\n",
      "     19        0.2126        \u001b[32m0.2696\u001b[0m     +  17.0469\n",
      "     20        \u001b[36m0.1719\u001b[0m        0.2793        17.1114\n",
      "     21        0.2223        0.2754        17.1156\n",
      "     22        0.2146        0.2828        17.1331\n",
      "     23        0.2138        0.2732        17.0585\n",
      "     24        0.1847        \u001b[32m0.2651\u001b[0m     +  17.1545\n",
      "     25        0.1947        \u001b[32m0.2609\u001b[0m     +  17.1459\n",
      "     26        0.2086        0.2688        17.1089\n",
      "     27        0.1749        0.2793        17.0772\n",
      "     28        \u001b[36m0.1696\u001b[0m        0.2706        17.1320\n",
      "     29        0.1774        \u001b[32m0.2574\u001b[0m     +  17.0899\n",
      "     30        \u001b[36m0.1579\u001b[0m        0.2657        17.0883\n",
      "     31        0.1777        0.2609        17.1018\n",
      "     32        0.2002        0.2621        17.1140\n",
      "     33        0.1627        \u001b[32m0.2570\u001b[0m     +  17.0532\n",
      "     34        0.1716        0.2879        17.1087\n",
      "     35        0.1901        0.2865        17.1495\n",
      "     36        0.1650        0.2887        17.1080\n",
      "     37        \u001b[36m0.1477\u001b[0m        \u001b[32m0.2508\u001b[0m     +  17.1089\n",
      "     38        0.1686        0.3074        17.0787\n",
      "     39        0.1667        0.2592        17.1558\n",
      "     40        0.1535        0.2615        17.0952\n",
      "     41        0.1616        0.2648        17.0752\n",
      "     42        \u001b[36m0.1385\u001b[0m        \u001b[32m0.2473\u001b[0m     +  17.1117\n",
      "     43        0.1926        0.2787        17.1885\n",
      "     44        0.1664        0.2704        17.1449\n",
      "     45        0.1751        0.2601        17.1316\n",
      "     46        0.1442        0.3049        17.2907\n",
      "     47        0.1452        0.2624        17.2527\n",
      "     48        0.1632        0.2542        17.2115\n",
      "     49        0.1459        0.2709        17.2131\n",
      "     50        0.1585        0.2920        17.2295\n",
      "     51        0.1509        \u001b[32m0.2289\u001b[0m     +  17.2279\n",
      "     52        0.1492        0.2847        17.2112\n",
      "     53        0.1592        0.2457        17.3079\n",
      "     54        0.1806        0.2460        17.2061\n",
      "     55        0.1561        0.2508        17.2168\n",
      "     56        0.1492        0.2409        17.2551\n",
      "     57        \u001b[36m0.1347\u001b[0m        0.2415        17.2704\n",
      "     58        0.1414        0.2438        17.2463\n",
      "     59        0.1577        0.2348        17.2233\n",
      "     60        0.1497        0.2594        17.2498\n",
      "     61        0.1427        0.2553        17.1851\n",
      "     62        0.1535        0.2471        17.2313\n",
      "     63        0.1501        0.2588        17.2716\n",
      "     64        \u001b[36m0.1164\u001b[0m        0.2395        17.2270\n",
      "     65        0.1443        0.2413        17.1947\n",
      "     66        0.1462        0.2513        17.2336\n",
      "     67        0.1565        0.2348        17.2382\n",
      "     68        0.1423        0.2357        17.1922\n",
      "     69        0.1254        \u001b[32m0.2218\u001b[0m     +  17.2105\n",
      "     70        0.1352        0.2378        17.2529\n",
      "     71        0.1423        0.2533        17.2254\n",
      "     72        0.1400        0.2616        17.1882\n",
      "     73        0.1384        0.2334        17.2359\n",
      "     74        0.1481        0.2391        17.2586\n",
      "     75        0.1339        0.2450        17.1724\n",
      "     76        0.1339        0.2235        17.2139\n",
      "     77        \u001b[36m0.1149\u001b[0m        0.2244        17.2974\n",
      "     78        \u001b[36m0.1048\u001b[0m        0.2233        17.2207\n",
      "     79        \u001b[36m0.1018\u001b[0m        0.2228        17.2599\n",
      "     80        0.1136        \u001b[32m0.2218\u001b[0m     +  17.2475\n",
      "     81        0.1230        \u001b[32m0.2169\u001b[0m     +  17.2434\n",
      "     82        \u001b[36m0.0984\u001b[0m        0.2186        17.1855\n",
      "     83        0.1010        \u001b[32m0.2160\u001b[0m     +  17.2441\n",
      "     84        0.1172        0.2168        17.1794\n",
      "     85        0.1134        0.2201        17.0687\n",
      "     86        0.1148        0.2170        17.1155\n",
      "     87        0.1093        0.2187        17.0848\n",
      "     88        0.1017        0.2185        17.1044\n",
      "     89        0.1060        \u001b[32m0.2153\u001b[0m     +  17.0495\n",
      "     90        0.1062        0.2162        17.0801\n",
      "     91        0.1236        0.2181        17.1348\n",
      "     92        \u001b[36m0.0977\u001b[0m        \u001b[32m0.2104\u001b[0m     +  17.0665\n",
      "     93        0.1028        0.2164        17.0992\n",
      "     94        0.1081        0.2120        17.0875\n",
      "     95        \u001b[36m0.0965\u001b[0m        0.2143        17.1229\n",
      "     96        0.1035        0.2157        17.0683\n",
      "     97        \u001b[36m0.0944\u001b[0m        0.2170        17.0721\n",
      "     98        0.0949        0.2161        17.1126\n",
      "     99        0.1177        0.2145        17.0907\n",
      "    100        \u001b[36m0.0897\u001b[0m        0.2115        17.1924\n",
      "Re-initializing module because the following parameters were re-set: atom_fea_len, classification, h_fea_len, n_conv, n_h, nbr_fea_len, orig_atom_fea_len.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss    cp      dur\n",
      "-------  ------------  ------------  ----  -------\n",
      "      1        \u001b[36m0.4037\u001b[0m        \u001b[32m0.3050\u001b[0m     +  17.2338\n",
      "      2        \u001b[36m0.3186\u001b[0m        \u001b[32m0.2812\u001b[0m     +  17.3427\n",
      "      3        \u001b[36m0.2693\u001b[0m        0.5289        17.1988\n",
      "      4        \u001b[36m0.2470\u001b[0m        0.3166        17.2092\n",
      "      5        \u001b[36m0.2192\u001b[0m        0.5291        17.2778\n",
      "      6        0.2537        \u001b[32m0.1870\u001b[0m     +  17.2201\n",
      "      7        0.2267        0.4074        17.1010\n",
      "      8        0.2471        0.2750        17.1002\n",
      "      9        \u001b[36m0.2181\u001b[0m        0.2551        17.1545\n",
      "     10        0.2233        0.1916        17.0944\n",
      "     11        \u001b[36m0.2173\u001b[0m        0.3731        17.0697\n",
      "     12        \u001b[36m0.2052\u001b[0m        0.2161        17.2000\n",
      "     13        0.2134        \u001b[32m0.1702\u001b[0m     +  17.1870\n",
      "     14        0.2130        0.2110        17.1091\n",
      "     15        0.2071        0.2504        17.1069\n",
      "     16        0.2209        \u001b[32m0.1562\u001b[0m     +  17.2133\n",
      "     17        \u001b[36m0.1988\u001b[0m        0.1702        17.1918\n",
      "     18        \u001b[36m0.1977\u001b[0m        0.1642        17.1176\n",
      "     19        \u001b[36m0.1949\u001b[0m        \u001b[32m0.1473\u001b[0m     +  17.1256\n",
      "     20        0.2050        0.1708        17.1516\n",
      "     21        0.1977        0.1797        17.1711\n",
      "     22        0.1993        0.1957        17.1155\n",
      "     23        0.2142        0.1605        17.1757\n",
      "     24        \u001b[36m0.1767\u001b[0m        0.2819        17.0896\n",
      "     25        \u001b[36m0.1732\u001b[0m        0.1600        17.2895\n",
      "     26        0.1848        0.2139        17.1536\n",
      "     27        0.2040        0.1552        17.1687\n",
      "     28        0.2048        0.2521        17.3621\n",
      "     29        0.1746        0.2063        17.2280\n",
      "     30        0.1746        \u001b[32m0.1434\u001b[0m     +  17.2114\n",
      "     31        0.1823        0.1544        17.0941\n",
      "     32        \u001b[36m0.1648\u001b[0m        \u001b[32m0.1434\u001b[0m     +  17.1429\n",
      "     33        0.1763        0.2005        17.1196\n",
      "     34        0.2016        0.1457        17.1363\n",
      "     35        \u001b[36m0.1647\u001b[0m        \u001b[32m0.1425\u001b[0m     +  17.1316\n",
      "     36        \u001b[36m0.1425\u001b[0m        0.1853        17.1149\n",
      "     37        0.2392        0.1635        17.1319\n",
      "     38        0.1898        0.1568        17.0957\n",
      "     39        0.1848        0.1526        17.0853\n",
      "     40        0.1907        0.1524        17.1121\n",
      "     41        0.1751        0.1788        17.1466\n",
      "     42        0.1682        0.1613        17.1089\n",
      "     43        0.1597        0.1625        17.1097\n",
      "     44        \u001b[36m0.1337\u001b[0m        0.1558        17.1645\n",
      "     45        0.1531        0.1628        17.1034\n",
      "     46        0.1629        0.1451        17.1115\n",
      "     47        0.1564        0.1719        17.1135\n",
      "     48        0.1578        0.1733        17.1271\n",
      "     49        0.1640        0.1642        17.1425\n",
      "     50        0.1483        0.2432        17.1423\n",
      "     51        0.1428        0.1440        17.1567\n",
      "     52        0.1691        0.1861        17.0884\n",
      "     53        0.1530        0.1688        17.1169\n",
      "     54        0.1502        0.1678        17.1156\n",
      "     55        0.1468        0.1591        17.1347\n",
      "     56        0.1463        0.1616        17.1248\n",
      "     57        0.1392        0.1571        17.1565\n",
      "     58        0.1558        0.1717        17.1468\n",
      "     59        0.1506        \u001b[32m0.1411\u001b[0m     +  19.3468\n",
      "     60        \u001b[36m0.1150\u001b[0m        0.1649        17.2553\n",
      "     61        0.1474        0.1480        17.2898\n",
      "     62        0.1516        0.1721        17.2428\n",
      "     63        0.1496        0.2298        17.2532\n",
      "     64        0.1606        0.1788        17.2247\n",
      "     65        0.1442        0.1553        17.2709\n",
      "     66        0.1382        0.1606        17.2682\n",
      "     67        0.1355        0.1481        17.2201\n",
      "     68        0.1528        0.1801        17.3068\n",
      "     69        0.1417        0.1437        17.2455\n",
      "     70        0.1423        0.1495        17.2589\n",
      "     71        0.1471        0.1639        17.2408\n",
      "     72        \u001b[36m0.1129\u001b[0m        0.1763        17.3069\n",
      "     73        0.1138        0.1537        17.2482\n",
      "     74        0.1381        0.1543        17.2275\n",
      "     75        0.1354        0.1672        17.2906\n",
      "     76        0.1378        0.1679        17.2358\n",
      "     77        0.1277        0.1465        17.2190\n",
      "     78        \u001b[36m0.1055\u001b[0m        0.1464        17.2431\n",
      "     79        0.1381        0.1477        17.2662\n",
      "     80        0.1127        0.1509        17.3315\n",
      "     81        0.1338        0.1512        17.2261\n",
      "     82        \u001b[36m0.0972\u001b[0m        0.1549        17.2939\n",
      "     83        0.1107        0.1532        17.2693\n",
      "     84        \u001b[36m0.0697\u001b[0m        0.1541        17.2099\n",
      "     85        0.1029        0.1561        17.2439\n",
      "     86        0.1210        0.1529        17.2875\n",
      "     87        0.1149        0.1579        17.2302\n",
      "     88        0.0972        0.1586        17.2265\n",
      "     89        0.1026        0.1564        17.2976\n",
      "     90        0.1165        0.1574        17.2508\n",
      "     91        0.1149        0.1600        17.2022\n",
      "     92        0.1015        0.1619        17.2690\n",
      "     93        0.1262        0.1619        17.2599\n",
      "     94        0.1003        0.1588        17.2401\n",
      "     95        0.1250        0.1569        17.2570\n",
      "     96        0.0955        0.1608        17.3008\n",
      "     97        0.1030        0.1656        17.2258\n",
      "     98        0.1047        0.1656        17.2177\n",
      "     99        0.0991        0.1678        17.1640\n",
      "    100        0.0964        0.1596        17.1119\n",
      "Re-initializing module because the following parameters were re-set: atom_fea_len, classification, h_fea_len, n_conv, n_h, nbr_fea_len, orig_atom_fea_len.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss    cp      dur\n",
      "-------  ------------  ------------  ----  -------\n",
      "      1        \u001b[36m0.3891\u001b[0m        \u001b[32m0.2972\u001b[0m     +  17.0888\n",
      "      2        \u001b[36m0.3429\u001b[0m        0.3022        17.0796\n",
      "      3        \u001b[36m0.2816\u001b[0m        0.4028        17.1723\n",
      "      4        \u001b[36m0.2569\u001b[0m        \u001b[32m0.2676\u001b[0m     +  17.0795\n",
      "      5        \u001b[36m0.2497\u001b[0m        \u001b[32m0.2245\u001b[0m     +  17.0827\n",
      "      6        \u001b[36m0.2381\u001b[0m        0.2704        17.0757\n",
      "      7        \u001b[36m0.2088\u001b[0m        \u001b[32m0.1734\u001b[0m     +  17.0873\n",
      "      8        0.2270        0.1919        17.0806\n",
      "      9        0.2176        0.2484        17.0559\n",
      "     10        0.2346        \u001b[32m0.1550\u001b[0m     +  17.1184\n",
      "     11        0.2356        0.1677        17.0701\n",
      "     12        \u001b[36m0.1991\u001b[0m        0.1897        17.1284\n",
      "     13        \u001b[36m0.1863\u001b[0m        0.2354        17.1749\n",
      "     14        \u001b[36m0.1765\u001b[0m        0.2014        17.1081\n",
      "     15        0.1985        0.1659        17.0653\n",
      "     16        0.2199        0.1809        17.0507\n",
      "     17        0.2060        0.2074        17.1163\n",
      "     18        0.2027        0.1633        17.0608\n",
      "     19        0.2042        0.1971        17.0676\n",
      "     20        0.2034        0.1684        17.0955\n",
      "     21        0.2016        0.1788        17.0779\n",
      "     22        0.2244        0.1709        17.0826\n",
      "     23        0.2005        0.1628        17.0636\n",
      "     24        0.1880        0.1847        17.1069\n",
      "     25        \u001b[36m0.1760\u001b[0m        0.1995        17.0569\n",
      "     26        0.1920        0.1690        17.0554\n",
      "     27        \u001b[36m0.1610\u001b[0m        0.1738        17.1184\n",
      "     28        0.2334        0.2141        17.0936\n",
      "     29        0.1946        0.1821        17.0560\n",
      "     30        0.1830        0.1994        17.0709\n",
      "     31        0.2004        0.1634        17.1912\n",
      "     32        0.1775        0.1996        17.0694\n",
      "     33        0.1743        \u001b[32m0.1459\u001b[0m     +  17.0408\n",
      "     34        0.1792        0.1715        17.2371\n",
      "     35        0.1754        0.2141        17.0586\n",
      "     36        0.1707        0.1890        17.0578\n",
      "     37        0.1864        0.2086        17.0915\n",
      "     38        0.1706        0.2031        17.0836\n",
      "     39        0.1891        0.1657        17.0993\n",
      "     40        0.1626        0.1668        17.0498\n",
      "     41        0.1787        0.1609        17.1189\n",
      "     42        0.1951        0.1540        17.0866\n",
      "     43        0.1669        \u001b[32m0.1363\u001b[0m     +  17.0309\n",
      "     44        0.1700        0.1542        17.0960\n",
      "     45        0.1892        0.1681        17.1117\n",
      "     46        0.1630        0.1891        17.0576\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim import Adam\n",
    "import skorch.callbacks.base\n",
    "from skorch.callbacks import Checkpoint  # needs skorch >= 0.4\n",
    "from skorch.callbacks.lr_scheduler import LRScheduler\n",
    "from skorch import NeuralNetRegressor\n",
    "from cgcnn.model import CrystalGraphConvNet\n",
    "from cgcnn.data import collate_pool, MergeDataset\n",
    "\n",
    "\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('./histories/%i_valid_best_params.pt' % k)\n",
    "\n",
    "nets = []\n",
    "# Fold the CV data\n",
    "k_folder = KFold(n_splits=5)\n",
    "for k, (indices_train, _) in enumerate(k_folder.split(sdts_train)):\n",
    "    stds_train_ = [sdts_train[index] for index in indices_train]\n",
    "    targets_train_ = np.array([targets_train[index] for index in indices_train])\n",
    "\n",
    "    # Define various callbacks and checkpointers for this network\n",
    "    LR_schedule = LRScheduler('MultiStepLR', milestones=[75], gamma=0.1)\n",
    "    cp = Checkpoint(monitor='valid_loss_best', fn_prefix='./histories/%i_valid_best_' % k)\n",
    "    load_best_valid_loss = train_end_load_best_valid_loss()\n",
    "\n",
    "    # Train this fold's network\n",
    "    net = NeuralNetRegressor(\n",
    "        CrystalGraphConvNet,\n",
    "        module__orig_atom_fea_len=orig_atom_fea_len,\n",
    "        module__nbr_fea_len=nbr_fea_len,\n",
    "        batch_size=214,\n",
    "        module__classification=False,\n",
    "        lr=0.0056,\n",
    "        max_epochs=100,\n",
    "        module__atom_fea_len=46,\n",
    "        module__h_fea_len=83,\n",
    "        module__n_conv=8,\n",
    "        module__n_h=4,\n",
    "        optimizer=Adam,\n",
    "        iterator_train__pin_memory=True,\n",
    "        iterator_train__num_workers=0,\n",
    "        iterator_train__collate_fn=collate_pool,\n",
    "        iterator_train__shuffle=True,\n",
    "        iterator_valid__pin_memory=True,\n",
    "        iterator_valid__num_workers=0,\n",
    "        iterator_valid__collate_fn=collate_pool,\n",
    "        iterator_valid__shuffle=False,\n",
    "        device=device,\n",
    "        criterion=torch.nn.L1Loss,\n",
    "        dataset=MergeDataset,\n",
    "        callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    "    )\n",
    "    net.initialize()\n",
    "    net.fit(stds_train_, targets_train_)\n",
    "    nets.append(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading models\n",
    "It takes a few hours to fit the 5-model ensemble. You can either do it via notebook (above) or via `sbatch submit_ensemble_fitting.sh`. Either way, you load the results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim import Adam\n",
    "import skorch.callbacks.base\n",
    "from skorch.callbacks import Checkpoint  # needs skorch >= 0.4\n",
    "from skorch.callbacks.lr_scheduler import LRScheduler\n",
    "from skorch import NeuralNetRegressor\n",
    "from cgcnn.model import CrystalGraphConvNet\n",
    "from cgcnn.data import collate_pool, MergeDataset\n",
    "\n",
    "\n",
    "class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "    def on_train_end(self, net, X, y):\n",
    "        net.load_params('./histories/%i_valid_best_params.pt' % k)\n",
    "\n",
    "nets = []\n",
    "# Fold the CV data\n",
    "k_folder = KFold(n_splits=5)\n",
    "for k, (indices_train, _) in enumerate(k_folder.split(sdts_train)):\n",
    "    stds_train_ = [sdts_train[index] for index in indices_train]\n",
    "    targets_train_ = np.array([targets_train[index] for index in indices_train])\n",
    "\n",
    "    # Define various callbacks and checkpointers for this network\n",
    "    LR_schedule = LRScheduler('MultiStepLR', milestones=[75], gamma=0.1)\n",
    "    cp = Checkpoint(monitor='valid_loss_best', fn_prefix='./histories/%i_valid_best_' % k)\n",
    "    load_best_valid_loss = train_end_load_best_valid_loss()\n",
    "\n",
    "    # Train this fold's network\n",
    "    net = NeuralNetRegressor(\n",
    "        CrystalGraphConvNet,\n",
    "        module__orig_atom_fea_len=orig_atom_fea_len,\n",
    "        module__nbr_fea_len=nbr_fea_len,\n",
    "        batch_size=214,\n",
    "        module__classification=False,\n",
    "        lr=0.0056,\n",
    "        max_epochs=100,\n",
    "        module__atom_fea_len=46,\n",
    "        module__h_fea_len=83,\n",
    "        module__n_conv=8,\n",
    "        module__n_h=4,\n",
    "        optimizer=Adam,\n",
    "        iterator_train__pin_memory=True,\n",
    "        iterator_train__num_workers=0,\n",
    "        iterator_train__collate_fn=collate_pool,\n",
    "        iterator_train__shuffle=True,\n",
    "        iterator_valid__pin_memory=True,\n",
    "        iterator_valid__num_workers=0,\n",
    "        iterator_valid__collate_fn=collate_pool,\n",
    "        iterator_valid__shuffle=False,\n",
    "        device=device,\n",
    "        criterion=torch.nn.L1Loss,\n",
    "        dataset=MergeDataset,\n",
    "        callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    "    )\n",
    "    net.initialize()\n",
    "    net.load_params(f_history='./histories/%i_valid_best_history.json' % k,\n",
    "                    f_optimizer= './histories/%i_valid_best_optimizer.pt' % k, \n",
    "                    f_params='./histories/%i_valid_best_params.pt' % k)\n",
    "    nets.append(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference - net.fit() vs net.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling\n",
    "Wrap the five networks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble:\n",
    "    def __init__(self, networks):\n",
    "        self.networks = networks\n",
    "\n",
    "    def predict(self, features):\n",
    "        for net in self.networks:\n",
    "            prediction = net.predict(features)\n",
    "            try:\n",
    "                predictions = np.hstack((predictions, prediction))\n",
    "            except NameError:\n",
    "                predictions = prediction\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "ensemble = Ensemble(nets)\n",
    "preds = ensemble.predict(sdts_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "# Make the predictions\n",
    "predictions = ensemble.predict(sdts_val)\n",
    "targets_pred = predictions.mean(axis=1)\n",
    "residuals = targets_pred - targets_val.reshape(-1)\n",
    "standard_errors = predictions.std(axis=1)\n",
    "\n",
    "# Calculate the error metrics\n",
    "mae = mean_absolute_error(targets_val, targets_pred)\n",
    "rmse = np.sqrt(mean_squared_error(targets_val, targets_pred))\n",
    "r2 = r2_score(targets_val, targets_pred)\n",
    "\n",
    "# Report\n",
    "print('MAE = %.2f eV' % mae)\n",
    "print('RMSE = %.2f eV' % rmse)\n",
    "print('R^2 = %.2f' % r2)\n",
    "\n",
    "# Save as pickle to be plotted with in the same graph as others\n",
    "with open('assess_ensemble_plots.pkl', 'wb') as saveplot:\n",
    "    pickle.dump((predictions, targets_pred, standard_errors, mae, rmse, r2), saveplot)\n",
    "    \n",
    "\"\"\"\n",
    "# Plot\n",
    "lims = [-4, 2]\n",
    "grid = sns.jointplot(targets_val.reshape(-1), targets_pred,\n",
    "                     kind='hex',\n",
    "                     bins='log',\n",
    "                     extent=lims+lims)\n",
    "_ = grid.ax_joint.set_xlim(lims)\n",
    "_ = grid.ax_joint.set_ylim(lims)\n",
    "_ = grid.ax_joint.plot(lims, lims, '--')\n",
    "_ = grid.ax_joint.set_xlabel('DFT $\\Delta$E [eV]')\n",
    "_ = grid.ax_joint.set_ylabel('CGCNN $\\Delta$E [eV]')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "def calculate_density(percentile):\n",
    "    num_within_quantile = 0\n",
    "    for se, resid in zip(standard_errors, residuals):\n",
    "        norm = stats.norm(loc=0, scale=se)\n",
    "        lower_bound = norm.ppf(0.5-percentile/2)\n",
    "        upper_bound = norm.ppf(0.5+percentile/2)\n",
    "        if lower_bound <= resid <= upper_bound:\n",
    "            num_within_quantile += 1\n",
    "    density = num_within_quantile / len(residuals)\n",
    "    return density\n",
    "\n",
    "predicted_pi = np.linspace(0, 1, 20)\n",
    "observed_pi = [calculate_density(quantile)\n",
    "               for quantile in tqdm_notebook(predicted_pi, desc='Calibration')]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import integrate\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Plot the calibration curve\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax_ideal = sns.lineplot([0, 1], [0, 1], label='ideal')\n",
    "_ = ax_ideal.lines[0].set_linestyle('--')\n",
    "ax_gp = sns.lineplot(predicted_pi, observed_pi, label='GP')\n",
    "_ = ax_gp.set_xlabel('Expected prediction interval')\n",
    "_ = ax_gp.set_ylabel('Observed prediction interval')\n",
    "_ = ax_gp.set_xlim([0, 1])\n",
    "_ = ax_gp.set_ylim([0, 1])\n",
    "\n",
    "# Report the calibration factor\n",
    "actual_calibration_area = integrate.trapz(y=observed_pi, x=predicted_pi)\n",
    "ideal_calibration_area = integrate.trapz(y=predicted_pi, x=predicted_pi)\n",
    "calibration_factor = actual_calibration_area - ideal_calibration_area\n",
    "print('Calibration factor = %.2f' % calibration_factor)\n",
    "\n",
    "# Report sharpness\n",
    "sharpness = np.sqrt(np.mean(standard_errors**2))\n",
    "print('Sharpness = %.2f eV' % sharpness)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaspy_ktran",
   "language": "python",
   "name": "gaspy_ktran"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
